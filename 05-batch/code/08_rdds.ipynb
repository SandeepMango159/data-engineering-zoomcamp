{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is for explaining the concepts of RDD's in spark and it's internals with code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeec39f",
   "metadata": {},
   "source": [
    "## Setup spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9447c87",
   "metadata": {},
   "source": [
    "### Setup python environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Sandeep SSD\\Programming SSD\\Data Engineering Zoomcamp\\data-engineering-zoomcamp\\dataenginzoomvenv\\Scripts\\python.exe\n",
      "c:\\Sandeep SSD\\Programming SSD\\Data Engineering Zoomcamp\\data-engineering-zoomcamp\\dataenginzoomvenv\\Scripts\\python.exe\n",
      "c:\\Sandeep SSD\\Programming SSD\\Data Engineering Zoomcamp\\data-engineering-zoomcamp\\dataenginzoomvenv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# Add the vevn to spark's settings, so inject the venvâ€™s Python into both driver & worker configs before recreating the session, to find the right python interpreter, since the notebook us running within the venv kernel, we can just use sys.executable\n",
    "import os, sys\n",
    "\n",
    "print(sys.executable)\n",
    "venv_python = sys.executable\n",
    "\n",
    "# 1) Ensure the worker uses exactly this Python executable:\n",
    "os.environ['PYSPARK_PYTHON'] = venv_python\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = venv_python\n",
    "\n",
    "print(os.environ['PYSPARK_PYTHON'])\n",
    "print(os.environ['PYSPARK_DRIVER_PYTHON'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4abd3",
   "metadata": {},
   "source": [
    "### Launch spark with those python variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .config(\"spark.pyspark.python\", venv_python) \\\n",
    "    .config(\"spark.pyspark.driver.python\", venv_python) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://192.168.0.181:4040\n"
     ]
    }
   ],
   "source": [
    "# Check which port the Spark UI is running on\n",
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646fc343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 2 * since we have year subfolders, and month subfolders, okay...\n",
    "df_green = spark.read.parquet('../../Data/data/csv/green/spark_parquet/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 12, 18, 15, 4), lpep_dropoff_datetime=datetime.datetime(2020, 1, 12, 18, 19, 52), store_and_fwd_flag='N', RatecodeID=1, PULocationID=41, DOLocationID=41, passenger_count=1, trip_distance=0.78, fare_amount=5.5, extra=0.0, mta_tax=0.5, tip_amount=1.58, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=7.88, payment_type=1, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 31, 20, 24, 10), lpep_dropoff_datetime=datetime.datetime(2020, 1, 31, 20, 31, 51), store_and_fwd_flag='N', RatecodeID=1, PULocationID=173, DOLocationID=70, passenger_count=1, trip_distance=0.98, fare_amount=7.0, extra=0.5, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=8.3, payment_type=2, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 7, 8, 16, 53), lpep_dropoff_datetime=datetime.datetime(2020, 1, 7, 8, 41, 39), store_and_fwd_flag='N', RatecodeID=1, PULocationID=74, DOLocationID=236, passenger_count=1, trip_distance=2.7, fare_amount=16.0, extra=0.0, mta_tax=0.5, tip_amount=3.91, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=23.46, payment_type=1, trip_type=1, congestion_surcharge=2.75),\n",
       " Row(VendorID=1, lpep_pickup_datetime=datetime.datetime(2020, 1, 15, 14, 47, 15), lpep_dropoff_datetime=datetime.datetime(2020, 1, 15, 14, 54, 34), store_and_fwd_flag='N', RatecodeID=1, PULocationID=25, DOLocationID=66, passenger_count=1, trip_distance=0.8, fare_amount=6.5, extra=0.0, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=7.3, payment_type=2, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=None, lpep_pickup_datetime=datetime.datetime(2020, 1, 31, 10, 8), lpep_dropoff_datetime=datetime.datetime(2020, 1, 31, 10, 20), store_and_fwd_flag=None, RatecodeID=None, PULocationID=259, DOLocationID=51, passenger_count=None, trip_distance=2.33, fare_amount=22.49, extra=2.75, mta_tax=0.0, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.3, total_amount=25.54, payment_type=None, trip_type=None, congestion_surcharge=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframes have rdd's in them \n",
    "df_green.rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196cccd5",
   "metadata": {},
   "source": [
    "### We are going to reproduce this query\n",
    "```\n",
    "SELECT \n",
    "    date_trunc('hour', lpep_pickup_datetime) AS hour, \n",
    "    PULocationID AS zone,\n",
    "\n",
    "    SUM(total_amount) AS amount,\n",
    "    COUNT(1) AS number_records\n",
    "FROM\n",
    "    green\n",
    "WHERE\n",
    "    lpep_pickup_datetime >= '2020-01-01 00:00:00'\n",
    "GROUP BY\n",
    "    1, 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74fe52cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# So select only those columns\n",
    "rdd = df_green \\\n",
    "    .select('lpep_pickup_datetime', 'PULocationID', 'total_amount') \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of the where filter using rdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a0bf382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa2b00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(year=2020, month=1, day=1)\n",
    "\n",
    "def filter_outliers(row):\n",
    "    return row.lpep_pickup_datetime >= start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69dd326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = rdd.take(10)\n",
    "row = rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd4b7006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 12, 18, 15, 4), PULocationID=41, total_amount=7.88)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining groupby filter in RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d99eb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncating the timestamp values grouping by and selecting the rows\n",
    "def prepare_for_grouping(row): \n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    key = (hour, zone)\n",
    "    \n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount, count)\n",
    "\n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb328a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will be used as the reducebykey method\n",
    "def calculate_revenue(left_value, right_value):\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "    \n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "    \n",
    "    return (output_amount, output_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ea260f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dae6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "RevenueRow = namedtuple('RevenueRow', ['hour', 'zone', 'revenue', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0a98ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap(row):\n",
    "    return RevenueRow(\n",
    "        hour=row[0][0], \n",
    "        zone=row[0][1],\n",
    "        revenue=row[1][0],\n",
    "        count=row[1][1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a09200b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c14d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = types.StructType([\n",
    "    types.StructField('hour', types.TimestampType(), True),\n",
    "    types.StructField('zone', types.IntegerType(), True),\n",
    "    types.StructField('revenue', types.DoubleType(), True),\n",
    "    types.StructField('count', types.IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56ea72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducebykey reduces duplicates values of the same key to only one\n",
    "# Unwrap to unnest the dataset\n",
    "df_result = rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF(result_schema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+-----+\n",
      "|               hour|zone|           revenue|count|\n",
      "+-------------------+----+------------------+-----+\n",
      "|2020-01-07 08:00:00|  74|1497.2599999999989|  104|\n",
      "|2020-01-08 20:00:00| 254|            126.62|    6|\n",
      "|2020-01-03 14:00:00| 242|            163.62|   11|\n",
      "|2020-01-31 21:00:00|  41| 588.1600000000001|   40|\n",
      "|2020-01-06 14:00:00|  95|511.40000000000003|   27|\n",
      "|2020-01-30 17:00:00| 247|              21.6|    2|\n",
      "|2020-01-05 04:00:00|  74|109.21999999999998|    7|\n",
      "|2020-01-05 13:00:00| 215|            515.66|   18|\n",
      "|2020-01-19 16:00:00| 181|            159.35|   14|\n",
      "|2020-01-18 21:00:00| 188|             52.26|    3|\n",
      "|2020-01-20 15:00:00|  25|            134.59|   12|\n",
      "|2020-01-04 12:00:00|  78|            146.93|    3|\n",
      "|2020-01-31 18:00:00|  75|1244.8699999999994|   89|\n",
      "|2020-01-20 01:00:00|  74|             42.32|    4|\n",
      "|2020-01-10 19:00:00|  74|1331.4899999999996|   89|\n",
      "|2020-01-31 23:00:00| 134|             57.32|    5|\n",
      "|2020-01-16 09:00:00|  82|            178.78|   18|\n",
      "|2020-01-14 21:00:00| 129|            145.52|   12|\n",
      "|2020-01-02 07:00:00| 169| 89.30000000000001|    3|\n",
      "|2020-01-12 19:00:00| 116|             26.06|    3|\n",
      "+-------------------+----+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4675bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.write.parquet('../../Data/data/module-05-batch/08rdds/tmp/green-revenue', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining mapPartition in RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imagine in Machine Learning we want to create a model that predicts based on the pickuptime the length of the duration of the trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "255b5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['VendorID', 'lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance']\n",
    "\n",
    "duration_rdd = df_green \\\n",
    "    .select(columns) \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "645c3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "921e4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = duration_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f50db3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b8ecc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'lpep_pickup_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'trip_distance']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6766c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ...\n",
    "\n",
    "def model_predict(df):\n",
    "#     y_pred = model.predict(df)\n",
    "    y_pred = df.trip_distance * 5\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7437b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the duration based on the trip pickup time\n",
    "# But uses dummy prediction method\n",
    "def apply_model_in_batch(rows):\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    predictions = model_predict(df)\n",
    "    df['predicted_duration'] = predictions\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict it\n",
    "df_predicts = duration_rdd \\\n",
    "    .mapPartitions(apply_model_in_batch)\\\n",
    "    .toDF() \\\n",
    "    .drop('Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055d543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|predicted_duration|\n",
      "+------------------+\n",
      "|3.9000000000000004|\n",
      "|               4.9|\n",
      "|              13.5|\n",
      "|               4.0|\n",
      "|             11.65|\n",
      "|13.100000000000001|\n",
      "|5.6499999999999995|\n",
      "| 6.800000000000001|\n",
      "|             55.75|\n",
      "|               8.9|\n",
      "|               5.0|\n",
      "|             13.75|\n",
      "|               5.5|\n",
      "|             19.05|\n",
      "|              9.25|\n",
      "|              45.7|\n",
      "|               5.2|\n",
      "| 5.699999999999999|\n",
      "|              5.75|\n",
      "|4.6000000000000005|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# And show the predicted duration\n",
    "df_predicts.select('predicted_duration').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91d243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataenginzoomvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
