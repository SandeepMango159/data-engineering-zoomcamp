{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for connecting your local Spark to GCS to read data from your datalake "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup python environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Sandeep SSD\\Programming SSD\\Data Engineering Zoomcamp\\data-engineering-zoomcamp\\dataenginzoomvenv\\Scripts\\python.exe\n",
      "c:\\Sandeep SSD\\Programming SSD\\Data Engineering Zoomcamp\\data-engineering-zoomcamp\\dataenginzoomvenv\\Scripts\\python.exe\n",
      "c:\\Sandeep SSD\\Programming SSD\\Data Engineering Zoomcamp\\data-engineering-zoomcamp\\dataenginzoomvenv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# Add the vevn to spark's settings, so inject the venvâ€™s Python into both driver & worker configs before recreating the session, to find the right python interpreter, since the notebook us running within the venv kernel, we can just use sys.executable\n",
    "import os, sys\n",
    "\n",
    "print(sys.executable)\n",
    "venv_python = sys.executable\n",
    "\n",
    "# 1) Ensure the worker uses exactly this Python executable:\n",
    "os.environ['PYSPARK_PYTHON'] = venv_python\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = venv_python\n",
    "\n",
    "print(os.environ['PYSPARK_PYTHON'])\n",
    "print(os.environ['PYSPARK_DRIVER_PYTHON'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch spark with those python variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Also add the jar for connecting to gcs, and the credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3307b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0ddbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_location = '../../secrets/terraform-runner/de-zoomcamp-course-458518-bf75c2c775e3.json'\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"../../lib/gcs-connector-hadoop3-latest.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the spark context\n",
    "#### Then we'll create a session from this context\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b83404e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Says whenever you see filesystem that starts with gs, you need to start this implementation, and also use these credentials\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now create the session from this config, with the venv too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4713e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .config(\"spark.pyspark.python\", venv_python) \\\n",
    "    .config(\"spark.pyspark.driver.python\", venv_python) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://192.168.0.181:4040\n"
     ]
    }
   ],
   "source": [
    "# Check which port the Spark UI is running on\n",
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read green files from GCP with the gcp link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee1eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark.read.parquet('gs://de-zoomcamp-course-458518-zoomcamp-bucket/spark/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show them to see it's correctly read\n",
    "df_green.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "104b40ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304517"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataenginzoomvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
