{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979d8f74",
   "metadata": {},
   "source": [
    "# This notebook is for executing an SQL query from week 4, but using spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d934522f",
   "metadata": {},
   "source": [
    "## The good thing about spark is that instead of writing everything in native SQL, like joins or merging different tables together, you can use spark and do it using code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0fc31d",
   "metadata": {},
   "source": [
    "## Setup spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25df6f0",
   "metadata": {},
   "source": [
    "### Setup python environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d193c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Sandeep SSD\\Programming SSD\\Data Engineering Zoomcamp\\data-engineering-zoomcamp\\dataenginzoomvenv\\Scripts\\python.exe\n",
      "c:\\Sandeep SSD\\Programming SSD\\Data Engineering Zoomcamp\\data-engineering-zoomcamp\\dataenginzoomvenv\\Scripts\\python.exe\n",
      "c:\\Sandeep SSD\\Programming SSD\\Data Engineering Zoomcamp\\data-engineering-zoomcamp\\dataenginzoomvenv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# Add the vevn to spark's settings, so inject the venvâ€™s Python into both driver & worker configs before recreating the session, to find the right python interpreter, since the notebook us running within the venv kernel, we can just use sys.executable\n",
    "import os, sys\n",
    "\n",
    "print(sys.executable)\n",
    "venv_python = sys.executable\n",
    "\n",
    "# 1) Ensure the worker uses exactly this Python executable:\n",
    "os.environ['PYSPARK_PYTHON'] = venv_python\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = venv_python\n",
    "\n",
    "print(os.environ['PYSPARK_PYTHON'])\n",
    "print(os.environ['PYSPARK_DRIVER_PYTHON'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70afbf",
   "metadata": {},
   "source": [
    "### Launch spark with those python variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3307b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .config(\"spark.pyspark.python\", venv_python) \\\n",
    "    .config(\"spark.pyspark.driver.python\", venv_python) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "610785cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which port the Spark UI is running on\n",
    "print(spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a44cb",
   "metadata": {},
   "source": [
    "## Read the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa423837",
   "metadata": {},
   "source": [
    "### Read green files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee1eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 2 * since we have year subfolders, and month subfolders, okay...\n",
    "df_green = spark.read.parquet('../../Data/data/csv/green/spark_parquet/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca5ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After reading the green parquet files, we show them\n",
    "df_green.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d714689b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304517"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "149bdae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', IntegerType(), True), StructField('lpep_pickup_datetime', TimestampType(), True), StructField('lpep_dropoff_datetime', TimestampType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('RatecodeID', IntegerType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('passenger_count', IntegerType(), True), StructField('trip_distance', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('ehail_fee', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('payment_type', IntegerType(), True), StructField('trip_type', IntegerType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the schemas, make sure it's the same as the one that we wrote to the parquet file\n",
    "df_green.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27ee0afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "649bb4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to make it easier to read\n",
    "df_green = df_green \\\n",
    "    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9034e1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show to make sure columns got renamed\n",
    "df_green.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff13f4ae",
   "metadata": {},
   "source": [
    "### Read yellow files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90cd6845",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet('../../Data/data/csv/yellow/spark_parquet/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f8b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After reading the green parquet files, we show them\n",
    "df_yellow.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6947a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39649199"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count just to make sure it's right\n",
    "df_yellow.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f006adde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', IntegerType(), True), StructField('tpep_pickup_datetime', TimestampType(), True), StructField('tpep_dropoff_datetime', TimestampType(), True), StructField('passenger_count', IntegerType(), True), StructField('trip_distance', DoubleType(), True), StructField('RatecodeID', IntegerType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('payment_type', IntegerType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the schemas, make sure it's the same as the one that we wrote to the parquet file\n",
    "df_yellow.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e689affe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yellow.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88822efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to make it easier to read, and also join with the green schema data\n",
    "df_yellow = df_yellow \\\n",
    "    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81f978ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show to make sure columns got renamed\n",
    "df_yellow.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f796a3e",
   "metadata": {},
   "source": [
    "### Joining both dataframes together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8605148a",
   "metadata": {},
   "source": [
    "#### Each dataframe has a columns field, which we can use to get the columns that both dataframes have in common, meaning which ones exist in both dataframes\n",
    "##### We have already renamed the pickup and dropoffdatetime columns for both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b230019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'ehail_fee',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'trip_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "610167a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we'll write a small function for that, that holds a common columns list\n",
    "# Puts the columns of yellow into a list\n",
    "# And iterate over the green columns, if they're also in the yellow columns set, we'll add them to the list\n",
    "# This way preserves the order of columns as defined by the spark dataframe when reading the parquet files\n",
    "common_colums = []\n",
    "\n",
    "yellow_columns = set(df_yellow.columns)\n",
    "\n",
    "for col in df_green.columns:\n",
    "    if col in yellow_columns:\n",
    "        common_colums.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "999a5421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "839d773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2498810a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, pickup_datetime: timestamp, dropoff_datetime: timestamp, store_and_fwd_flag: string, RatecodeID: int, PULocationID: int, DOLocationID: int, passenger_count: int, trip_distance: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, payment_type: int, congestion_surcharge: double, service_type: string]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using select we can select the list of common columns, \n",
    "# We'll also add a service type to distinguish where they come from\n",
    "# F.lit for literal\n",
    "df_green_sel = df_green \\\n",
    "    .select(common_colums) \\\n",
    "    .withColumn('service_type', F.lit('green'))\n",
    "\n",
    "# Print to make sure it works\n",
    "df_green_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19032efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, pickup_datetime: timestamp, dropoff_datetime: timestamp, store_and_fwd_flag: string, RatecodeID: int, PULocationID: int, DOLocationID: int, passenger_count: int, trip_distance: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, payment_type: int, congestion_surcharge: double, service_type: string]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same for this yellow dataframe\n",
    "df_yellow_sel = df_yellow \\\n",
    "    .select(common_colums) \\\n",
    "    .withColumn('service_type', F.lit('yellow'))\n",
    "\n",
    "df_yellow_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5b0f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join them together\n",
    "df_trips_data = df_green_sel.unionAll(df_yellow_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d96d28fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'congestion_surcharge',\n",
       " 'service_type']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all the columns again\n",
    "df_trips_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1bed8b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|service_type|   count|\n",
      "+------------+--------+\n",
      "|       green| 2304517|\n",
      "|      yellow|39649199|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the rows by grouping by service type to make sure they're the same as before \n",
    "df_trips_data.groupBy('service_type').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c846bcc",
   "metadata": {},
   "source": [
    "### Writing the SQL code for spark dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91d70c",
   "metadata": {},
   "source": [
    "##### First we have to be able to use the dataframe, we do that by registering it as a table, otherwise SQL statements won't work on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "36e90cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a table for it, but this method is deprecated so use the new method below\n",
    "# df_trips_data.registerTempTable('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e619b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new suggested method to use instead\n",
    "df_trips_data.createOrReplaceTempView('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0e01bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|service_type|count(1)|\n",
      "+------------+--------+\n",
      "|       green| 2304517|\n",
      "|      yellow|39649199|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we can write sql and access that dataframe by the table name, results are the same as above\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    service_type,\n",
    "    count(1)\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY \n",
    "    service_type\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e132449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the same query from week 4, for dm_monthly_zone_revenue\n",
    "\n",
    "df_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    -- Revenue grouping \n",
    "    PULocationID AS revenue_zone,\n",
    "    date_trunc('month', pickup_datetime) AS revenue_month, \n",
    "    service_type, \n",
    "\n",
    "    -- Revenue calculation \n",
    "    SUM(fare_amount) AS revenue_monthly_fare,\n",
    "    SUM(extra) AS revenue_monthly_extra,\n",
    "    SUM(mta_tax) AS revenue_monthly_mta_tax,\n",
    "    SUM(tip_amount) AS revenue_monthly_tip_amount,\n",
    "    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n",
    "    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n",
    "    SUM(total_amount) AS revenue_monthly_total_amount,\n",
    "    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n",
    "\n",
    "    -- Additional calculations\n",
    "    AVG(passenger_count) AS avg_monthly_passenger_count,\n",
    "    AVG(trip_distance) AS avg_monthly_trip_distance\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY\n",
    "    1, 2, 3\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c90f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+---------------------------+-------------------------+\n",
      "|revenue_zone|      revenue_month|service_type|revenue_monthly_fare|revenue_monthly_extra|revenue_monthly_mta_tax|revenue_monthly_tip_amount|revenue_monthly_tolls_amount|revenue_monthly_improvement_surcharge|revenue_monthly_total_amount|revenue_monthly_congestion_surcharge|avg_monthly_passenger_count|avg_monthly_trip_distance|\n",
      "+------------+-------------------+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+---------------------------+-------------------------+\n",
      "|          47|2020-01-01 00:00:00|       green|  16630.150000000023|               1192.0|                  160.0|        42.230000000000004|           523.0000000000002|                   210.90000000000015|          18770.429999999975|                                11.0|          1.177685950413223|        4.645000000000002|\n",
      "|          53|2020-01-01 00:00:00|       green|   6281.649999999996|               330.25|                   49.0|                     39.22|          102.50000000000001|                    68.39999999999988|           6873.770000000004|                                2.75|         1.2417582417582418|        6.929710743801654|\n",
      "|         128|2020-01-01 00:00:00|       green|   3261.330000000001|                158.0|                   35.5|                     82.25|          104.03999999999999|                    34.50000000000004|          3714.0200000000004|                                27.5|         1.0169491525423728|        7.023280000000001|\n",
      "|         150|2020-01-01 00:00:00|       green|             9996.16|                627.5|                   98.0|         97.86999999999998|                      149.14|                   109.49999999999962|          11082.069999999992|                                 0.0|         1.1147540983606556|        5.393588516746414|\n",
      "|         263|2020-01-01 00:00:00|       green|   8987.339999999998|                561.0|                 124.55|                    315.27|           335.0600000000001|                    127.1999999999995|          10684.170000000004|                              239.25|         1.4293478260869565|        5.264249422632793|\n",
      "|          19|2020-01-01 00:00:00|       green|   7798.879999999999|                377.0|                   42.0|                      6.66|          174.40000000000003|                     67.4999999999999|           8466.440000000004|                                 0.0|                   1.046875|        7.850781893004114|\n",
      "|         186|2020-01-01 00:00:00|       green|             1539.23|                85.25|                    6.0|                       0.0|           97.91999999999999|                                 13.8|          1742.1999999999998|                                 0.0|         1.6666666666666667|        9.389565217391304|\n",
      "|         238|2020-01-01 00:00:00|       green|   5397.990000000001|                403.5|                   20.0|                       0.0|                      124.68|                    61.19999999999993|           6007.370000000001|                                 0.0|                        1.2|        6.460536585365853|\n",
      "|         126|2020-01-01 00:00:00|       green|   11827.83999999999|               766.75|                  110.5|        101.86999999999999|           458.2800000000002|                   145.19999999999933|          13425.340000000004|                                11.0|                      1.275|        5.187806691449811|\n",
      "|          96|2020-01-01 00:00:00|       green|   628.8000000000001|                 27.5|                    4.0|                       0.0|                       55.08|                    5.699999999999999|           721.0799999999999|                                 0.0|         1.1428571428571428|       7.0195454545454545|\n",
      "|         116|2020-01-01 00:00:00|       green|   78706.70999999996|              3319.25|                 2632.5|         5847.950000000004|          1467.9299999999985|                   1756.4999999999027|           96138.29000000306|                              2793.0|          1.212485896953742|        3.068908940397343|\n",
      "|          90|2020-01-01 00:00:00|       green|             2921.62|                165.0|                   19.0|                       0.0|          190.56000000000006|                   25.800000000000022|          3321.9800000000005|                                 0.0|                        1.2|        8.451627906976745|\n",
      "|         101|2020-01-01 00:00:00|       green|   8472.769999999997|                460.5|                   38.0|        15.219999999999999|          232.56000000000006|                    70.49999999999986|           9289.550000000001|                                2.75|         1.0512820512820513|        9.081153846153853|\n",
      "|         227|2020-01-01 00:00:00|       green|  14857.100000000011|               776.75|                   94.5|        21.569999999999997|          271.57000000000005|                   137.09999999999937|          16173.289999999986|                                 0.0|         1.0743801652892562|        7.235522088353416|\n",
      "|         180|2020-01-01 00:00:00|       green|  4024.4600000000005|               210.75|                   26.0|                     20.24|           84.88999999999999|                    43.20000000000006|           4414.240000000002|                                2.75|         1.3396226415094339|        5.324473684210526|\n",
      "|         190|2020-01-01 00:00:00|       green|  3786.0199999999995|                175.0|                   76.5|        242.16000000000008|                       24.48|                    62.69999999999993|           4419.360000000001|                               35.75|         1.4244604316546763|       3.5506912442396326|\n",
      "|         264|2008-12-01 00:00:00|       green|                 0.0|                  0.0|                    0.0|                       0.0|                         0.0|                                  0.0|                         0.0|                                 0.0|                        1.0|                      0.0|\n",
      "|         119|2020-01-01 00:00:00|       green|  21224.650000000012|               1391.0|                  186.5|                     74.62|           730.0800000000004|                    245.7000000000011|           23870.19999999996|                               13.75|         1.1612903225806452|        5.585108695652177|\n",
      "|          26|2020-01-01 00:00:00|       green|   27098.79000000005|              1743.25|                  221.5|        121.72999999999999|           551.2800000000002|                   292.50000000000233|           30034.89999999993|                                 0.0|         1.1054852320675106|        4.771315789473685|\n",
      "|         120|2020-01-01 00:00:00|       green|   692.4300000000001|                 5.25|                    7.0|        21.970000000000002|                       12.24|                    8.399999999999999|           752.7899999999998|                                 5.5|         1.1935483870967742|                4.6365625|\n",
      "+------------+-------------------+------------+--------------------+---------------------+-----------------------+--------------------------+----------------------------+-------------------------------------+----------------------------+------------------------------------+---------------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result to check it outselves...\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2ab0b9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9895"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count to see how many rows we have, must have a row for each locationid, month and service type combination\n",
    "df_result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67eeb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And write out the fie\n",
    "# Use coalesce, it's like partition, but to reduce partitions, so it'll reduce it to 1 partition\n",
    "df_result.coalesce(1).write.parquet('../../Data/data/module-05-batch/06-spark-sql/report/revenue', mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataenginzoomvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
